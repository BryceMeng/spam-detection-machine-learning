{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam detection using ML\n",
    "\n",
    "Hao Meng 2020.7\n",
    "\n",
    "Rewrite in the notebook in 2021.10\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In my previous notebook, I analyzed how to detect malware using Supervised learning of machine learning. In this notebook, I will discuss how to detect spam emails using machine learning. \n",
    "\n",
    "Detection of email is different from malware as email only has some words. \n",
    "\n",
    "In research from Graham (2002), he got a high accuracy of spam email detection by Naive Bayesian filter. I will briefly explain the principle of the Naive Bayesian filter. \n",
    "\n",
    "**1. Bayesian Inference**\n",
    "\n",
    "In general, it is conditional probability. For example. In total, the probability of rain in tomorrow is 0.3. However, if many clouds appear in today's evening, the probability of rain in tomorrow will increase to 0.5. The condition is that colouds appear in the evening of previous day.\n",
    "\n",
    "In spam email detection, we assume the probability of the spam email is 0.5. However, if \"sex\" appears in email, the probability of the spam email will increase to 0.8. We can statstic the probability of some key words similar to \"sex\" from all samples emails.\n",
    "\n",
    "**2. Naive Bayesian**\n",
    "\n",
    "It can't just judge whether it is a spam email by one word. There are more keywords like \"Gamble,\" \"smoke,\" and so on. If every word's conditional probability is independent, the Bayesian Inference method will be transferred to the Naive Bayesian Inference method. And the function of the combine probabilities is:\n",
    "\n",
    "![Combine Probabilities](https://chart.googleapis.com/chart?cht=tx&chl=P%3D%5Cfrac%7BP_%7B1%7DP_%7B2%7D%5Ccdot%20%5Ccdot%20%5Ccdot%20P_%7B15%7D%7D%7BP_%7B1%7DP_%7B2%7D%5Ccdot%20%5Ccdot%20%5Ccdot%20P_%7B15%7D%2B(1-P_%7B1%7D)(1-P_%7B2%7D)%5Ccdot%20%5Ccdot%20%5Ccdot%20(1-P_%7B15%7D)%7D&chs=70)\n",
    "\n",
    "The P with a subscript represents one word's condition probability, and in the function, there are 15 words to calc the spam probability together. Later more words can be added to the function.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "This notebook will use Ling-Spam Dataset provided by Ion Androutsopoulos(https://www.kaggle.com/mandygu/lingspam-dataset?rvi=1).\n",
    "\n",
    "\n",
    "### Reference Materials\n",
    "\n",
    "In this notebook, I will use the feature engineering method mainly from the notebook(https://www.kaggle.com/surekharamireddy/spam-detection-with-99-accuracy).\n",
    "\n",
    "### Flow\n",
    "\n",
    "1. Feature engineering\n",
    "2. Vectorlize\n",
    "3. Analysing by some models\n",
    "4. Analysing by Naive Bayesian model\n",
    "\n",
    "\n",
    "**Reference**\n",
    "Graham, P. (2002, August). A Plan for Spam. Paul Graham. http://www.paulgraham.com/spam.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, \\\n",
    "    ExtraTreesClassifier\n",
    "from collections import Counter\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "BASE_DIR = \"/media/xueshan/WD_BLACK/cybersecurity/spam\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"lingspam-dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>job posting - apple-iss research center</td>\n",
       "      <td>content - length : 3386 apple-iss research cen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>lang classification grimes , joseph e . and ba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>query : letter frequencies for text identifica...</td>\n",
       "      <td>i am posting this inquiry for sergei atamas ( ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>risk</td>\n",
       "      <td>a colleague and i are researching the differin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>request book information</td>\n",
       "      <td>earlier this morning i was on the phone with a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             subject  \\\n",
       "0            job posting - apple-iss research center   \n",
       "1                                                NaN   \n",
       "2  query : letter frequencies for text identifica...   \n",
       "3                                               risk   \n",
       "4                           request book information   \n",
       "\n",
       "                                             message  label  \n",
       "0  content - length : 3386 apple-iss research cen...      0  \n",
       "1  lang classification grimes , joseph e . and ba...      0  \n",
       "2  i am posting this inquiry for sergei atamas ( ...      0  \n",
       "3  a colleague and i are researching the differin...      0  \n",
       "4  earlier this morning i was on the phone with a...      0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_file = os.path.join(DATA_DIR, \"messages.csv\")\n",
    "df=pd.read_csv(db_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message'] = df['message'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject    62\n",
       "message     0\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check null data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject    0\n",
       "message    0\n",
       "label      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fillna(\" \", inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2412\n",
       "1     481\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine subject and message\n",
    "df['integration_msg']=df['subject']+df['message']\n",
    "df.drop('subject',axis=1,inplace=True)\n",
    "\n",
    "df['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vectorlize\n",
    "\n",
    "This phase is crucial. Text data is different from other data like integer, float, which ML models can't use. It has to be transferred into a vector. \n",
    "\n",
    "The target vector should be based on the word frequent. For example, the number of times sex appears in an email is a usable feature. But first, the email should be processed to be more readable by the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def decontact(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "df['integration_msg']=df['integration_msg'].apply(decontact)\n",
    "#the number is useless\n",
    "df['integration_msg']=df['integration_msg'].str.replace(r'\\d+(\\.\\d+)?', 'numbers')\n",
    "#Converting message to lowercase\n",
    "df['integration_msg']=df['integration_msg'].str.lower()\n",
    "# replacing line break with ' '\n",
    "df['integration_msg']=df['integration_msg'].str.replace(r'\\n',\" \") \n",
    "# replacing email \n",
    "# df['integration_msg']=df['integration_msg'].str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$','MailID')\n",
    "# replacing urls \n",
    "# df['integration_msg']=df['integration_msg'].str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$','Links')\n",
    "# replacing currency signs by 'Money', we don't care about the number\n",
    "df['integration_msg']=df['integration_msg'].str.replace(r'Â£|\\$', 'Money')\n",
    "# replacing large white space by single white space\n",
    "df['integration_msg']=df['integration_msg'].str.replace(r'\\s+', ' ')\n",
    "\n",
    "# replacing leading and trailing white space by single white space\n",
    "df['integration_msg']=df['integration_msg'].str.replace(r'^\\s+|\\s+?$', '')\n",
    "# replacing contact numbers \n",
    "# df['integration_msg']=df['integration_msg'].str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$','contact number')\n",
    "# replacing special characters by ' '\n",
    "df['integration_msg']=df['integration_msg'].str.replace(r\"[^a-zA-Z0-9]+\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lang classification grimes joseph e and barbara f grimes ethnologue language family index pb isbn numbers numbers numbers numbers vi numbers pp Money numbers numbers summer institute of linguistics this companion volume to ethnologue languages of the world twelfth edition lists language families of the world with sub groups shown in a tree arrangement under the broadest classification of language family the language family index facilitates locating language names in the ethnologue making the data there more accessible internet academic books sil org languages reference lang culture gregerson marilyn ritual belief and kinship in sulawesi pb isbn numbers numbers numbers numbers ix numbers pp Money numbers numbers summer institute of linguistics seven articles discuss five language groups in sulawesi indonesia the primary focus is on cultural matters with some linguistic content topics include traditional religion and beliefs certain ceremonies and kinship internet academic books sil org language and society indonesia computers ling weber david j stephen r mcconnel diana d weber and beth j bryson primer a tool for developing early reading materials pb isbn numbers numbers numbers numbers xvi numbers pp ms dos software Money numbers numbers summer institute of linguistics the authors present a computer program and instructions for developing reading materials in languages with little or no background in literacy the book is structured as a how to manual with step by step procedures to establish an appropriate primer sequence and to organize words phrases and sentences that correlate with the sequence it presupposes a thorough knowledge of linguistics internet academic books sil org literacy computer'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['integration_msg'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to remove stop words which has no contribution to detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# removing stopwords \n",
    "stop = stopwords.words('english')\n",
    "df['concise_text'] = df['integration_msg'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('message',axis=1,inplace=True)\n",
    "df.drop('integration_msg',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>concise_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>job posting apple iss research centercontent l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>lang classification grimes joseph e barbara f ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>query letter frequencies text identificationi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>riska colleague researching differing degrees ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>request book informationearlier morning phone ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                       concise_text\n",
       "0      0  job posting apple iss research centercontent l...\n",
       "1      0  lang classification grimes joseph e barbara f ...\n",
       "2      0  query letter frequencies text identificationi ...\n",
       "3      0  riska colleague researching differing degrees ...\n",
       "4      0  request book informationearlier morning phone ..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()\n",
    "df_vec = tvec.fit_transform(df.concise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2893x56901 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 508795 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After vectorlization, the number of features has increased to 56901."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysing by some models\n",
    "\n",
    "- Logistic\n",
    "- Ada Boost\n",
    "- Gradient Boosting\n",
    "- ExtraTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X = df_vec\n",
    "Y = df.label\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 225,stratify=Y)\n",
    "\n",
    "def output_test_result(model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    confusion_matrix(y_pred,Y_test)\n",
    "\n",
    "    print(\"Accuracy : \", accuracy_score(y_pred,Y_test))\n",
    "    print(\"Precision : \", precision_score(y_pred,Y_test, average = 'weighted'))\n",
    "    print(\"Recall : \", recall_score(y_pred,Y_test, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced', n_jobs=14)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(class_weight='balanced', n_jobs=14)\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9913644214162349\n",
      "Precision :  0.9915695834540169\n",
      "Recall :  0.9913644214162349\n"
     ]
    }
   ],
   "source": [
    "output_test_result(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(n_estimators=500)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = AdaBoostClassifier(n_estimators=500)\n",
    "abc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9896373056994818\n",
      "Precision :  0.9897669287734618\n",
      "Recall :  0.9896373056994818\n"
     ]
    }
   ],
   "source": [
    "output_test_result(abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(n_estimators=500)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=500)\n",
    "gbc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9792746113989638\n",
      "Precision :  0.9812178400683695\n",
      "Recall :  0.9792746113989638\n"
     ]
    }
   ],
   "source": [
    "output_test_result(gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTrees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(class_weight='balanced', max_features=None,\n",
       "                     min_samples_split=9, n_estimators=500, n_jobs=14)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etc = ExtraTreesClassifier(n_estimators=500, max_features=None, min_samples_leaf=1,\n",
    "                                                      min_samples_split=9, n_jobs=14,\n",
    "                                                      class_weight=\"balanced\",\n",
    "                                                      criterion='gini')\n",
    "etc.fit(X_train, Y_train)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9913644214162349\n",
      "Precision :  0.9913391672656148\n",
      "Recall :  0.9913644214162349\n"
     ]
    }
   ],
   "source": [
    "output_test_result(etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysing by Naive Bayesian model\n",
    "\n",
    "There is a trap in the Naive Bayesian model. The naive Bayesian model can't accept the vector in fd-idf, which only can accept the vector in simple counter formal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2893x56901 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 508795 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = CountVectorizer()\n",
    "X = tf.fit_transform(df.concise_text)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df.label\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 225,stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB(fit_prior=True)\n",
    "mnb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9896373056994818\n",
      "Precision :  0.9896368587233648\n",
      "Recall :  0.9896373056994818\n"
     ]
    }
   ],
   "source": [
    "output_test_result(mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
